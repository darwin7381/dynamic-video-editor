#!/usr/bin/env python3
"""
å°ˆæ¥­èªéŸ³è½‰æ–‡å­—æ¨¡å‹æ¸¬è©¦
ä½¿ç”¨é‡å°æ€§çš„ Prompt è§£æ±º SRT æ®µè½éé•·å•é¡Œ
"""

import os
from dotenv import load_dotenv
import time
import json
from openai import OpenAI

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

def format_srt_time(seconds):
    """å°‡ç§’æ•¸è½‰æ›ç‚º SRT æ™‚é–“æ ¼å¼"""
    hours = int(seconds // 3600)
    minutes = int((seconds % 3600) // 60)
    secs = int(seconds % 60)
    millisecs = int((seconds % 1) * 1000)
    return f"{hours:02d}:{minutes:02d}:{secs:02d},{millisecs:03d}"

def create_srt_from_segments(segments):
    """å¾æ®µè½è³‡è¨Šå‰µå»º SRT"""
    srt_content = []
    
    for i, segment in enumerate(segments, 1):
        start_time = format_srt_time(segment.start)
        end_time = format_srt_time(segment.end)
        text = segment.text.strip()
        
        srt_content.append(f"{i}")
        srt_content.append(f"{start_time} --> {end_time}")
        srt_content.append(text)
        srt_content.append("")
    
    return "\n".join(srt_content)

def analyze_srt_readability(srt_content, model_name):
    """å¯¦éš›åˆ†æ SRT çš„å¯è®€æ€§å’Œå­—å¹•å“è³ª"""
    print(f"\nğŸ“– {model_name} - å­—å¹•å¯è®€æ€§åˆ†æ")
    print("=" * 60)
    
    lines = srt_content.strip().split('\n')
    segments = []
    current_segment = {"id": None, "time": None, "text": ""}
    
    for line in lines:
        if line.strip().isdigit():
            if current_segment["text"]:
                segments.append(current_segment.copy())
            current_segment = {"id": int(line), "time": None, "text": ""}
        elif "-->" in line:
            current_segment["time"] = line.strip()
        elif line.strip():
            current_segment["text"] += line + " "
    
    if current_segment["text"]:
        segments.append(current_segment)
    
    # å¯¦éš›å…§å®¹åˆ†æ
    print(f"ğŸ“Š å­—å¹•çµ±è¨ˆ:")
    print(f"  ç¸½æ®µè½æ•¸: {len(segments)}")
    
    segment_lengths = [len(seg["text"].strip()) for seg in segments]
    avg_length = sum(segment_lengths) / len(segment_lengths)
    max_length = max(segment_lengths)
    min_length = min(segment_lengths)
    
    print(f"  å¹³å‡æ®µè½é•·åº¦: {avg_length:.1f} å­—ç¬¦")
    print(f"  æœ€é•·æ®µè½: {max_length} å­—ç¬¦")
    print(f"  æœ€çŸ­æ®µè½: {min_length} å­—ç¬¦")
    
    # å­—å¹•å¯è®€æ€§è©•ä¼°
    ideal_segments = [s for s in segment_lengths if 15 <= s <= 35]  # ç†æƒ³çš„å­—å¹•é•·åº¦
    too_short = [s for s in segment_lengths if s < 10]
    too_long = [s for s in segment_lengths if s > 40]
    
    print(f"\nğŸ“ˆ å­—å¹•å¯è®€æ€§è©•ä¼°:")
    print(f"  ç†æƒ³é•·åº¦ (15-35å­—ç¬¦): {len(ideal_segments)} å€‹ ({len(ideal_segments)/len(segments)*100:.1f}%)")
    print(f"  éçŸ­ (<10å­—ç¬¦): {len(too_short)} å€‹ ({len(too_short)/len(segments)*100:.1f}%)")
    print(f"  éé•· (>40å­—ç¬¦): {len(too_long)} å€‹ ({len(too_long)/len(segments)*100:.1f}%)")
    
    # åˆ†æèªç¾©å®Œæ•´æ€§
    complete_sentences = 0
    has_punctuation = 0
    
    print(f"\nğŸ“ èªç¾©å“è³ªåˆ†æ:")
    print(f"å¯¦éš›å­—å¹•å…§å®¹ç¤ºä¾‹:")
    
    for i, seg in enumerate(segments[:8]):  # åˆ†æå‰8å€‹æ®µè½
        text = seg["text"].strip()
        time_info = seg["time"]
        
        # æª¢æŸ¥å®Œæ•´æ€§å’Œæ¨™é»ç¬¦è™Ÿ
        is_complete = text.endswith(('ã€‚', 'ï¼', 'ï¼Ÿ', '.', '!', '?', 'ï¼Œ', ','))
        has_punct = any(p in text for p in ['ï¼Œ', 'ã€‚', 'ï¼', 'ï¼Ÿ', ',', '.', '!', '?', 'ã€'])
        
        if is_complete:
            complete_sentences += 1
        if has_punct:
            has_punctuation += 1
        
        # è©•ä¼°é€™å€‹æ®µè½çš„å“è³ª
        length_score = "âœ…" if 15 <= len(text) <= 35 else "âš ï¸" if len(text) > 40 else "ğŸ”¸"
        complete_score = "âœ…" if is_complete else "âŒ"
        punct_score = "âœ…" if has_punct else "âŒ"
        
        print(f"  {i+1}. [{time_info}] ({len(text)}å­—ç¬¦) {length_score}")
        print(f"     å…§å®¹: {text}")
        print(f"     å®Œæ•´æ€§: {complete_score} | æ¨™é»: {punct_score}")
        print()
    
    completion_rate = complete_sentences / len(segments) * 100
    punctuation_rate = has_punctuation / len(segments) * 100
    
    print(f"ğŸ“Š æ•´é«”å“è³ªæŒ‡æ¨™:")
    print(f"  èªç¾©å®Œæ•´åº¦: {completion_rate:.1f}% ({complete_sentences}/{len(segments)})")
    print(f"  æ¨™é»ç¬¦è™Ÿç‡: {punctuation_rate:.1f}% ({has_punctuation}/{len(segments)})")
    
    # å­—å¹•é¡¯ç¤ºé©ç”¨æ€§è©•ä¼°
    readability_score = (len(ideal_segments)/len(segments) * 0.4 + 
                        completion_rate/100 * 0.3 + 
                        punctuation_rate/100 * 0.3)
    
    print(f"  å­—å¹•å¯è®€æ€§è©•åˆ†: {readability_score*100:.1f}/100")
    
    if readability_score > 0.8:
        quality_level = "ğŸ† å„ªç§€"
    elif readability_score > 0.6:
        quality_level = "âœ… è‰¯å¥½"
    elif readability_score > 0.4:
        quality_level = "âš ï¸ ä¸€èˆ¬"
    else:
        quality_level = "âŒ éœ€æ”¹å–„"
    
    print(f"  æ•´é«”è©•ç´š: {quality_level}")
    
    return {
        'total_segments': len(segments),
        'avg_length': avg_length,
        'max_length': max_length,
        'min_length': min_length,
        'ideal_segments': len(ideal_segments),
        'too_short': len(too_short),
        'too_long': len(too_long),
        'completion_rate': completion_rate,
        'punctuation_rate': punctuation_rate,
        'readability_score': readability_score * 100,
        'quality_level': quality_level
    }

def test_model_with_professional_prompt(client, model, audio_file, base_url=None):
    """ä½¿ç”¨å°ˆæ¥­ Prompt æ¸¬è©¦æ¨¡å‹"""
    
    if base_url:
        client = OpenAI(api_key=client.api_key, base_url=base_url)
    
    # é‡å°æ‚¨çš„å•é¡Œè¨­è¨ˆçš„å°ˆæ¥­ Prompt
    professional_prompt = """è«‹å°‡èªéŸ³è½‰éŒ„ç‚ºé©åˆå­—å¹•é¡¯ç¤ºçš„æ ¼å¼ã€‚æ¯å€‹å­—å¹•æ®µè½æ‡‰è©²ï¼š
1. é•·åº¦æ§åˆ¶åœ¨ 20-35 å€‹å­—ç¬¦ä¹‹é–“
2. åœ¨èªç¾©å®Œæ•´çš„åœ°æ–¹è‡ªç„¶æ–·å¥
3. é¿å…åœ¨è©å½™ä¸­é–“æ–·é–‹
4. ä¿æŒæ™‚é–“åŒæ­¥çš„æº–ç¢ºæ€§
5. æ·»åŠ é©ç•¶çš„æ¨™é»ç¬¦è™Ÿ
é€™æ˜¯ä¸€æ®µè²¡ç¶“æ–°èå…§å®¹ï¼ŒåŒ…å«è‚¡å¸‚ã€å…¬å¸åç¨±ç­‰å°ˆæ¥­è¡“èªã€‚"""
    
    results = []
    
    # æ¸¬è©¦ä¸åŒçš„é…ç½®
    test_configs = [
        ("ç„¡ Prompt", "", None),
        ("å°ˆæ¥­å­—å¹• Prompt", professional_prompt, None),
        ("æ®µè½ç´šæ™‚é–“æˆ³è¨˜", professional_prompt, ["segment"]),
        ("è©å½™ç´šæ™‚é–“æˆ³è¨˜", professional_prompt, ["word"]),
        ("æ®µè½+è©å½™ç´š", professional_prompt, ["segment", "word"])
    ]
    
    for config_name, prompt, timestamp_granularities in test_configs:
        print(f"\n--- æ¸¬è©¦é…ç½®: {config_name} ---")
        
        try:
            start_time = time.time()
            
            # æ§‹å»º API åƒæ•¸
            params = {
                "model": model,
                "file": open(audio_file, "rb"),
                "language": "zh"
            }
            
            # æ·»åŠ  prompt
            if prompt:
                params["prompt"] = prompt
            
            # æ ¹æ“šæ¨¡å‹æ±ºå®šæ ¼å¼å’Œæ™‚é–“æˆ³è¨˜
            if "gpt-4o" in model:
                params["response_format"] = "verbose_json"  # å˜—è©¦ verbose_json
                if timestamp_granularities:
                    params["timestamp_granularities"] = timestamp_granularities
            elif model == "whisper-1":
                params["response_format"] = "verbose_json"
                if timestamp_granularities:
                    params["timestamp_granularities"] = timestamp_granularities
            elif "whisper-large-v3" in model:
                params["response_format"] = "verbose_json"
            
            transcription = client.audio.transcriptions.create(**params)
            
            processing_time = time.time() - start_time
            
            print(f"âœ… æˆåŠŸ - è™•ç†æ™‚é–“: {processing_time:.2f} ç§’")
            
            # åˆ†æçµæœ
            text = transcription.text if hasattr(transcription, 'text') else str(transcription)
            print(f"ğŸ“ æ–‡å­—é•·åº¦: {len(text)} å­—ç¬¦")
            print(f"ğŸŒ æª¢æ¸¬èªè¨€: {transcription.language if hasattr(transcription, 'language') else 'N/A'}")
            
            # æª¢æŸ¥æ™‚é–“æˆ³è¨˜æ”¯æ´
            has_segments = hasattr(transcription, 'segments') and transcription.segments
            has_words = hasattr(transcription, 'words') and transcription.words
            
            if has_segments:
                print(f"ğŸ“Š æ®µè½æ•¸: {len(transcription.segments)}")
            if has_words:
                print(f"ğŸ“ è©å½™æ•¸: {len(transcription.words)}")
            
            # å‰µå»º SRT (å¦‚æœæœ‰æ®µè½è³‡è¨Š)
            srt_content = None
            if has_segments:
                srt_content = create_srt_from_segments(transcription.segments)
                
                # ä¿å­˜ SRT
                filename = f"{model.replace('-', '_').replace('/', '_')}_{config_name.replace(' ', '_').replace('+', '_')}.srt"
                with open(filename, "w", encoding="utf-8") as f:
                    f.write(srt_content)
                print(f"ğŸ’¾ SRT å·²ä¿å­˜: {filename}")
            
            # é¡¯ç¤ºæ–‡å­—å“è³ª
            print(f"ğŸ“– æ–‡å­—å“è³ªé è¦½:")
            print(f"  {text[:150]}...")
            
            results.append({
                'model': model,
                'config': config_name,
                'prompt': prompt,
                'timestamp_granularities': timestamp_granularities,
                'success': True,
                'processing_time': processing_time,
                'text': text,
                'has_segments': has_segments,
                'has_words': has_words,
                'segment_count': len(transcription.segments) if has_segments else 0,
                'word_count': len(transcription.words) if has_words else 0,
                'srt_content': srt_content
            })
            
        except Exception as e:
            print(f"âŒ å¤±æ•— - éŒ¯èª¤: {str(e)}")
            results.append({
                'model': model,
                'config': config_name,
                'success': False,
                'error': str(e)
            })
        
        time.sleep(1)  # é¿å… API é™åˆ¶
    
    return results

def main():
    """ä¸»æ¸¬è©¦å‡½æ•¸"""
    print("ğŸ¯ å°ˆæ¥­èªéŸ³è½‰æ–‡å­—æ¨¡å‹æ¸¬è©¦ - è§£æ±º SRT æ®µè½éé•·å•é¡Œ")
    print("=" * 80)
    
    # API é‡‘é‘°
    openai_api_key = os.getenv("OPENAI_API_KEY")
    groq_api_key = os.getenv("GROQ_API_KEY")
    
    audio_file = "test_audio.mp3"
    
    if not os.path.exists(audio_file):
        print(f"âŒ éŒ¯èª¤: æ‰¾ä¸åˆ°éŸ³æª” {audio_file}")
        return
    
    # åˆå§‹åŒ–å®¢æˆ¶ç«¯
    openai_client = OpenAI(api_key=openai_api_key)
    groq_client = OpenAI(
        api_key=groq_api_key,
        base_url="https://api.groq.com/openai/v1"
    )
    
    all_results = []
    
    # 1. æ¸¬è©¦ OpenAI whisper-1
    print(f"\nğŸ” ç¬¬ä¸€éƒ¨åˆ†ï¼šOpenAI Whisper-1 å°ˆæ¥­æ¸¬è©¦")
    whisper1_results = test_model_with_professional_prompt(openai_client, "whisper-1", audio_file)
    all_results.extend(whisper1_results)
    
    # 2. é‡æ–°æ¸¬è©¦ OpenAI æ–°æ¨¡å‹ (ä½¿ç”¨æ­£ç¢ºçš„åƒæ•¸)
    print(f"\nğŸ” ç¬¬äºŒéƒ¨åˆ†ï¼šOpenAI æ–°æ¨¡å‹å°ˆæ¥­æ¸¬è©¦")
    for model in ["gpt-4o-transcribe", "gpt-4o-mini-transcribe"]:
        print(f"\nğŸš€ æ¸¬è©¦ {model}")
        model_results = test_model_with_professional_prompt(openai_client, model, audio_file)
        all_results.extend(model_results)
    
    # 3. æ¸¬è©¦ Groq Whisper Large v3
    print(f"\nğŸ” ç¬¬ä¸‰éƒ¨åˆ†ï¼šGroq Whisper Large v3 å°ˆæ¥­æ¸¬è©¦")
    groq_results = test_model_with_professional_prompt(groq_client, "whisper-large-v3", audio_file, "https://api.groq.com/openai/v1")
    all_results.extend(groq_results)
    
    # 4. æ·±åº¦åˆ†ææ‰€æœ‰ç”Ÿæˆçš„ SRT
    print(f"\n" + "=" * 80)
    print(f"ğŸ“– SRT å­—å¹•å“è³ªæ·±åº¦åˆ†æ")
    print("=" * 80)
    
    srt_analyses = {}
    successful_srt_results = [r for r in all_results if r['success'] and r.get('srt_content')]
    
    for result in successful_srt_results:
        analysis_key = f"{result['model']}_{result['config']}"
        analysis = analyze_srt_readability(result['srt_content'], analysis_key)
        srt_analyses[analysis_key] = analysis
        
        # ä¿å­˜åˆ†æçµæœ
        result['srt_analysis'] = analysis
    
    # 5. ç¶œåˆæ¯”è¼ƒå’Œæ¨è–¦
    print(f"\n" + "=" * 80)
    print(f"ğŸ† ç¶œåˆæ¯”è¼ƒèˆ‡æ¨è–¦")
    print("=" * 80)
    
    if srt_analyses:
        print(f"ğŸ“Š æ¨¡å‹è¡¨ç¾æ’è¡Œæ¦œ (æŒ‰å­—å¹•å¯è®€æ€§è©•åˆ†):")
        
        # æŒ‰å¯è®€æ€§è©•åˆ†æ’åº
        sorted_analyses = sorted(srt_analyses.items(), key=lambda x: x[1]['readability_score'], reverse=True)
        
        for i, (model_config, analysis) in enumerate(sorted_analyses, 1):
            print(f"  {i}. {model_config}")
            print(f"     å¯è®€æ€§è©•åˆ†: {analysis['readability_score']:.1f}/100 {analysis['quality_level']}")
            print(f"     ç†æƒ³æ®µè½æ¯”ä¾‹: {analysis['ideal_segments']}/{analysis['total_segments']} ({analysis['ideal_segments']/analysis['total_segments']*100:.1f}%)")
            print(f"     èªç¾©å®Œæ•´åº¦: {analysis['completion_rate']:.1f}%")
            print(f"     æ¨™é»ç¬¦è™Ÿç‡: {analysis['punctuation_rate']:.1f}%")
            print()
        
        # æ‰¾å‡ºæœ€ä½³è§£æ±ºæ–¹æ¡ˆ
        best_overall = sorted_analyses[0]
        print(f"ğŸ… æœ€ä½³æ•´é«”è¡¨ç¾: {best_overall[0]}")
        print(f"   è©•åˆ†: {best_overall[1]['readability_score']:.1f}/100")
        
        # é‡å°ä¸åŒéœ€æ±‚çš„æ¨è–¦
        print(f"\nğŸ¯ é‡å°ä¸åŒéœ€æ±‚çš„æ¨è–¦:")
        
        # æœ€çŸ­æ®µè½
        shortest_avg = min(srt_analyses.items(), key=lambda x: x[1]['avg_length'])
        print(f"  æœ€çŸ­æ®µè½: {shortest_avg[0]} (å¹³å‡ {shortest_avg[1]['avg_length']:.1f} å­—ç¬¦)")
        
        # æœ€å°‘éé•·æ®µè½
        least_long = min(srt_analyses.items(), key=lambda x: x[1]['too_long'])
        print(f"  æœ€å°‘éé•·æ®µè½: {least_long[0]} ({least_long[1]['too_long']} å€‹éé•·æ®µè½)")
        
        # æœ€é«˜å®Œæ•´åº¦
        highest_completion = max(srt_analyses.items(), key=lambda x: x[1]['completion_rate'])
        print(f"  æœ€é«˜èªç¾©å®Œæ•´åº¦: {highest_completion[0]} ({highest_completion[1]['completion_rate']:.1f}%)")
    
    # 6. ä¿å­˜å®Œæ•´çµæœ
    with open("professional_model_test_results.json", "w", encoding="utf-8") as f:
        json.dump({
            'test_results': all_results,
            'srt_analyses': srt_analyses
        }, f, ensure_ascii=False, indent=2, default=str)
    
    print(f"\nğŸ’¾ å®Œæ•´æ¸¬è©¦çµæœå·²ä¿å­˜åˆ°: professional_model_test_results.json")
    print("ğŸ‰ å°ˆæ¥­æ¸¬è©¦å®Œæˆï¼")

if __name__ == "__main__":
    main()
