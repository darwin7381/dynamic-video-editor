#!/usr/bin/env python3
"""
ä½¿ç”¨ç¾æœ‰æ•¸æ“šé€²è¡Œæ®µè½ç´š vs è©å½™ç´šæ·±åº¦åˆ†æ
"""

import json
import os

def analyze_srt_file(filepath, name):
    """åˆ†æ SRT æ–‡ä»¶çš„æ®µè½ç‰¹æ€§"""
    try:
        with open(filepath, "r", encoding="utf-8") as f:
            content = f.read()
        
        # è§£æ SRT æ®µè½
        blocks = content.strip().split('\n\n')
        segments = []
        
        for block in blocks:
            lines = block.split('\n')
            if len(lines) >= 3:
                text = lines[2]  # ç¬¬ä¸‰è¡Œæ˜¯æ–‡æœ¬
                segments.append(text)
        
        if not segments:
            return None
        
        lengths = [len(seg) for seg in segments]
        long_segments = sum(1 for length in lengths if length > 25)
        very_long = sum(1 for length in lengths if length > 35)
        
        analysis = {
            'name': name,
            'total_segments': len(segments),
            'avg_length': sum(lengths) / len(lengths),
            'max_length': max(lengths),
            'min_length': min(lengths),
            'long_segments': long_segments,
            'very_long_segments': very_long,
            'score': max(0, 60 - long_segments * 5 - very_long * 10),
            'segments': segments[:5]  # å‰5å€‹æ®µè½ä½œç‚ºæ¨£æœ¬
        }
        
        return analysis
        
    except Exception as e:
        print(f"âŒ ç„¡æ³•åˆ†æ {filepath}: {str(e)}")
        return None

def analyze_json_segments(filepath, name):
    """åˆ†æ JSON æ–‡ä»¶ä¸­çš„æ®µè½æ•¸æ“š"""
    try:
        with open(filepath, "r", encoding="utf-8") as f:
            data = json.load(f)
        
        segments = []
        
        # æª¢æŸ¥ä¸åŒçš„æ®µè½æ•¸æ“šçµæ§‹
        if 'segments' in data and data['segments']:
            for seg in data['segments']:
                if 'text' in seg:
                    segments.append(seg['text'])
        elif 'words' in data:
            # å¾è©å½™ç´šæ•¸æ“šæ¨æ–·æ®µè½
            words = data['words']
            current_segment = ""
            
            for word in words:
                current_segment += word.get('text', '')
                
                # ç°¡å–®çš„æ®µè½åˆ†å‰²é‚è¼¯
                if len(current_segment) > 30 or word.get('text', '') in ['ã€‚', 'ï¼', 'ï¼Ÿ']:
                    if current_segment.strip():
                        segments.append(current_segment.strip())
                        current_segment = ""
            
            if current_segment.strip():
                segments.append(current_segment.strip())
        
        if not segments:
            return None
        
        lengths = [len(seg) for seg in segments]
        long_segments = sum(1 for length in lengths if length > 25)
        very_long = sum(1 for length in lengths if length > 35)
        
        analysis = {
            'name': name,
            'total_segments': len(segments),
            'avg_length': sum(lengths) / len(lengths),
            'max_length': max(lengths),
            'min_length': min(lengths),
            'long_segments': long_segments,
            'very_long_segments': very_long,
            'score': max(0, 60 - long_segments * 5 - very_long * 10),
            'segments': segments[:5]
        }
        
        return analysis
        
    except Exception as e:
        print(f"âŒ ç„¡æ³•åˆ†æ {filepath}: {str(e)}")
        return None

def create_comprehensive_comparison():
    """å‰µå»ºå…¨é¢çš„æ®µè½ç´š vs è©å½™ç´šæ¯”è¼ƒ"""
    print("ğŸ¯ æ®µè½ç´š vs è©å½™ç´šå…¨é¢æ¯”è¼ƒåˆ†æ")
    print("=" * 80)
    
    analyses = []
    
    # ElevenLabs æ®µè½ç´š (å¾è½‰æ›å¾Œçš„çµæœ)
    elevenlabs_segment = analyze_srt_file("elevenlabs_segment_level.srt", "ElevenLabs æ®µè½ç´š")
    if elevenlabs_segment:
        analyses.append(elevenlabs_segment)
    
    # ElevenLabs è©å½™ç´š (æœ€ä½³ç‰ˆæœ¬)
    elevenlabs_word = analyze_srt_file("elevenlabs_precise_18chars.srt", "ElevenLabs è©å½™ç´š")
    if elevenlabs_word:
        analyses.append(elevenlabs_word)
    
    # AssemblyAI è©å½™ç´š
    assemblyai_word = analyze_srt_file("assemblyai_precise_18chars.srt", "AssemblyAI è©å½™ç´š")
    if assemblyai_word:
        analyses.append(assemblyai_word)
    
    # Groq è©å½™ç´š
    groq_word = analyze_srt_file("final_groq_word_level.srt", "Groq è©å½™ç´š")
    if groq_word:
        analyses.append(groq_word)
    
    # Whisper-1 è©å½™ç´š
    whisper_word = analyze_srt_file("final_whisper1_word_level_correct.srt", "Whisper-1 è©å½™ç´š")
    if whisper_word:
        analyses.append(whisper_word)
    
    # å˜—è©¦å¾ JSON æ•¸æ“šåˆ†æåŸå§‹æ®µè½ç´š
    elevenlabs_json_segment = analyze_json_segments("elevenlabs_segment_result.json", "ElevenLabs åŸå§‹æ®µè½ç´š")
    if elevenlabs_json_segment:
        analyses.append(elevenlabs_json_segment)
    
    # é¡¯ç¤ºæ¯”è¼ƒè¡¨æ ¼
    print(f"\nğŸ“Š è©³ç´°æ¯”è¼ƒçµæœ:")
    print(f"{'æ–¹æ¡ˆ':<20} {'æ®µè½æ•¸':<8} {'å¹³å‡é•·åº¦':<8} {'æœ€é•·':<6} {'å•é¡Œæ®µè½':<8} {'è©•åˆ†':<6}")
    print("-" * 70)
    
    for analysis in analyses:
        print(f"{analysis['name']:<20} {analysis['total_segments']:<8} "
              f"{analysis['avg_length']:<8.1f} {analysis['max_length']:<6} "
              f"{analysis['long_segments']:<8} {analysis['score']:<6.0f}")
    
    # åˆ†çµ„æ¯”è¼ƒ
    print(f"\nğŸ” åˆ†çµ„æ¯”è¼ƒåˆ†æ:")
    
    # ElevenLabs æ®µè½ç´š vs è©å½™ç´š
    elevenlabs_comparisons = [a for a in analyses if 'ElevenLabs' in a['name']]
    if len(elevenlabs_comparisons) >= 2:
        print(f"\n  ğŸ“ˆ ElevenLabs æ¯”è¼ƒ:")
        for comp in elevenlabs_comparisons:
            print(f"    {comp['name']}: æœ€é•· {comp['max_length']} å­—ç¬¦, å•é¡Œæ®µè½ {comp['long_segments']} å€‹")
    
    # è©å½™ç´šæœå‹™æ¯”è¼ƒ
    word_level_comparisons = [a for a in analyses if 'è©å½™ç´š' in a['name']]
    if word_level_comparisons:
        print(f"\n  ğŸ† è©å½™ç´šæœå‹™æ’å:")
        word_level_comparisons.sort(key=lambda x: x['score'], reverse=True)
        for i, comp in enumerate(word_level_comparisons, 1):
            print(f"    {i}. {comp['name']}: {comp['score']:.0f}/60 (æœ€é•· {comp['max_length']} å­—ç¬¦)")
    
    # é¡¯ç¤ºæ¨£æœ¬æ®µè½
    print(f"\nğŸ“ æ¨£æœ¬æ®µè½æ¯”è¼ƒ:")
    for analysis in analyses[:3]:  # åªé¡¯ç¤ºå‰3å€‹
        print(f"\n  {analysis['name']} å‰3å€‹æ®µè½:")
        for i, seg in enumerate(analysis['segments'][:3], 1):
            print(f"    {i}. {seg} ({len(seg)} å­—ç¬¦)")
    
    return analyses

def generate_final_recommendations(analyses):
    """åŸºæ–¼åˆ†æçµæœç”Ÿæˆæœ€çµ‚å»ºè­°"""
    print(f"\nğŸ¯ æœ€çµ‚å»ºè­°")
    print("=" * 60)
    
    # æ‰¾å‡ºæœ€ä½³è©å½™ç´šæ–¹æ¡ˆ
    word_level = [a for a in analyses if 'è©å½™ç´š' in a['name']]
    if word_level:
        best_word = max(word_level, key=lambda x: x['score'])
        print(f"ğŸ† æœ€ä½³è©å½™ç´šæ–¹æ¡ˆ: {best_word['name']}")
        print(f"   è©•åˆ†: {best_word['score']:.0f}/60")
        print(f"   æœ€é•·æ®µè½: {best_word['max_length']} å­—ç¬¦")
        print(f"   å•é¡Œæ®µè½: {best_word['long_segments']} å€‹")
    
    # æ®µè½ç´š vs è©å½™ç´šç¸½çµ
    segment_level = [a for a in analyses if 'æ®µè½ç´š' in a['name']]
    
    print(f"\nğŸ“‹ æ®µè½ç´š vs è©å½™ç´šç¸½çµ:")
    print(f"  æ®µè½ç´šç‰¹é»:")
    print(f"    âœ… é©åˆé–±è®€å’Œç†è§£")
    print(f"    âœ… èªç¾©å®Œæ•´æ€§å¥½")
    print(f"    âŒ æ®µè½é•·åº¦ä¸å¯æ§")
    print(f"    âŒ ä¸é©åˆå­—å¹•é¡¯ç¤º")
    
    print(f"  è©å½™ç´šç‰¹é»:")
    print(f"    âœ… ç²¾ç¢ºæ§åˆ¶æ®µè½é•·åº¦")
    print(f"    âœ… é©åˆå­—å¹•å’Œè¦–é »")
    print(f"    âœ… å¯è‡ªå®šç¾©åˆ†å‰²é‚è¼¯")
    print(f"    âŒ éœ€è¦é¡å¤–è™•ç†")
    
    print(f"\nğŸ‰ æœ€çµ‚çµè«–:")
    print(f"  å°æ–¼æ‚¨çš„éœ€æ±‚ï¼ˆè§£æ±ºæ®µè½éé•·å•é¡Œï¼‰ï¼Œè©å½™ç´šæ˜é¡¯å„ªæ–¼æ®µè½ç´š")
    print(f"  ElevenLabs Scribe V1 + è©å½™ç´šè‡ªå®šç¾©ä»æ˜¯æœ€ä½³è§£æ±ºæ–¹æ¡ˆ")

def main():
    """ä¸»åˆ†æå‡½æ•¸"""
    print("ğŸ” ä½¿ç”¨ç¾æœ‰æ•¸æ“šé€²è¡Œæ®µè½ç´š vs è©å½™ç´šæ·±åº¦åˆ†æ")
    print("=" * 80)
    
    # æª¢æŸ¥å¯ç”¨æ–‡ä»¶
    available_files = []
    test_files = [
        ("elevenlabs_segment_level.srt", "ElevenLabs æ®µè½ç´š"),
        ("elevenlabs_precise_18chars.srt", "ElevenLabs è©å½™ç´š"),
        ("assemblyai_precise_18chars.srt", "AssemblyAI è©å½™ç´š"),
        ("final_groq_word_level.srt", "Groq è©å½™ç´š"),
        ("final_whisper1_word_level_correct.srt", "Whisper-1 è©å½™ç´š")
    ]
    
    print(f"ğŸ“‹ æª¢æŸ¥å¯ç”¨æ–‡ä»¶:")
    for filepath, name in test_files:
        if os.path.exists(filepath):
            print(f"  âœ… {name}: {filepath}")
            available_files.append((filepath, name))
        else:
            print(f"  âŒ {name}: {filepath} (ä¸å­˜åœ¨)")
    
    if not available_files:
        print(f"âŒ æ²’æœ‰æ‰¾åˆ°å¯åˆ†æçš„æ–‡ä»¶")
        return
    
    # åŸ·è¡Œå…¨é¢æ¯”è¼ƒ
    analyses = create_comprehensive_comparison()
    
    # ç”Ÿæˆæœ€çµ‚å»ºè­°
    generate_final_recommendations(analyses)
    
    # æ›´æ–° TODO ç‹€æ…‹
    print(f"\nâœ… æ®µè½ç´š vs è©å½™ç´šæ¯”è¼ƒåˆ†æå®Œæˆ")

if __name__ == "__main__":
    main()
