#!/usr/bin/env python3
"""
å…¬å¹³æ¯”è¼ƒæ¸¬è©¦ - å»é™¤ç°¡ç¹é«”æ‰£åˆ†ï¼Œå°ˆæ³¨æ–¼å¯¦éš›èƒ½åŠ›
ç¢ºå¯¦æ¸¬è©¦è©å½™ç´šæ™‚é–“æˆ³è¨˜æ”¯æ´
"""

import os
import json

def format_srt_time(seconds):
    """å°‡ç§’æ•¸è½‰æ›ç‚º SRT æ™‚é–“æ ¼å¼"""
    hours = int(seconds // 3600)
    minutes = int((seconds % 3600) // 60)
    secs = int(seconds % 60)
    millisecs = int((seconds % 1) * 1000)
    return f"{hours:02d}:{minutes:02d}:{secs:02d},{millisecs:03d}"

def test_word_level_capability(words, service_name, word_format="elevenlabs"):
    """æ¸¬è©¦è©å½™ç´šæ™‚é–“æˆ³è¨˜èƒ½åŠ›"""
    print(f"\nğŸ” {service_name} è©å½™ç´šæ™‚é–“æˆ³è¨˜æ¸¬è©¦")
    print("=" * 60)
    
    if not words:
        print(f"âŒ {service_name} æ²’æœ‰è©å½™ç´šæ™‚é–“æˆ³è¨˜")
        return False
    
    print(f"âœ… {service_name} æ”¯æ´è©å½™ç´šæ™‚é–“æˆ³è¨˜: {len(words)} å€‹è©å½™")
    
    # é¡¯ç¤ºå‰10å€‹è©å½™çš„æ™‚é–“æˆ³è¨˜
    print(f"ğŸ“ å‰10å€‹è©å½™æ™‚é–“æˆ³è¨˜:")
    for i, word in enumerate(words[:10]):
        if word_format == "elevenlabs":
            word_text = word['text']
            start_time = word['start']
            end_time = word['end']
        else:  # assemblyai
            word_text = word.text if hasattr(word, 'text') else str(word)
            start_time = word.start / 1000 if hasattr(word, 'start') else 0
            end_time = word.end / 1000 if hasattr(word, 'end') else 0
        
        print(f"  {i+1:2d}. [{start_time:.3f}-{end_time:.3f}] '{word_text}'")
    
    # æ¸¬è©¦ä¸åŒé•·åº¦çš„ SRT ç”Ÿæˆ
    test_lengths = [15, 18, 20, 25]
    best_results = []
    
    for target_length in test_lengths:
        print(f"\nğŸ“Š æ¸¬è©¦ç›®æ¨™é•·åº¦: {target_length} å­—ç¬¦")
        
        segments = []
        current_segment = {'start': None, 'end': None, 'text': ''}
        
        for word in words:
            if word_format == "elevenlabs":
                word_text = word['text'].strip()
                start_time = word['start']
                end_time = word['end']
            else:
                word_text = word.text.strip() if hasattr(word, 'text') else str(word).strip()
                start_time = word.start / 1000 if hasattr(word, 'start') else 0
                end_time = word.end / 1000 if hasattr(word, 'end') else 0
            
            if not word_text:
                continue
                
            if current_segment['start'] is None:
                current_segment['start'] = start_time
                current_segment['end'] = end_time
                current_segment['text'] = word_text
            elif len(current_segment['text'] + word_text) > target_length:
                segments.append(current_segment.copy())
                current_segment = {
                    'start': start_time,
                    'end': end_time,
                    'text': word_text
                }
            else:
                current_segment['text'] += word_text
                current_segment['end'] = end_time
        
        if current_segment['text']:
            segments.append(current_segment)
        
        # åˆ†æé€™å€‹é•·åº¦è¨­å®šçš„æ•ˆæœ
        if segments:
            lengths = [len(seg['text']) for seg in segments]
            max_length = max(lengths)
            avg_length = sum(lengths) / len(lengths)
            problem_count = sum(1 for l in lengths if l > 30)
            
            print(f"  çµæœ: {len(segments)} æ®µè½ï¼Œæœ€é•· {max_length} å­—ç¬¦ï¼Œå¹³å‡ {avg_length:.1f} å­—ç¬¦")
            print(f"  å•é¡Œæ®µè½: {problem_count} å€‹")
            
            # ç”Ÿæˆä¸¦ä¿å­˜ SRT
            srt_lines = []
            for i, seg in enumerate(segments, 1):
                start_time = format_srt_time(seg['start'])
                end_time = format_srt_time(seg['end'])
                
                srt_lines.append(f"{i}")
                srt_lines.append(f"{start_time} --> {end_time}")
                srt_lines.append(seg['text'])
                srt_lines.append("")
            
            srt_content = '\n'.join(srt_lines)
            filename = f"{service_name.lower().replace(' ', '_')}_word_level_{target_length}chars.srt"
            
            with open(filename, "w", encoding="utf-8") as f:
                f.write(srt_content)
            print(f"  ğŸ’¾ å·²ä¿å­˜: {filename}")
            
            best_results.append({
                'target_length': target_length,
                'max_length': max_length,
                'avg_length': avg_length,
                'problem_count': problem_count,
                'segment_count': len(segments),
                'filename': filename
            })
    
    # æ‰¾å‡ºæœ€ä½³è¨­å®š
    if best_results:
        # æŒ‰å•é¡Œæ®µè½æ•¸å’Œæœ€é•·æ®µè½æ’åº
        best_results.sort(key=lambda x: (x['problem_count'], x['max_length']))
        best = best_results[0]
        
        print(f"\nğŸ† {service_name} æœ€ä½³è©å½™ç´šè¨­å®š:")
        print(f"  ç›®æ¨™é•·åº¦: {best['target_length']} å­—ç¬¦")
        print(f"  å¯¦éš›æœ€é•·: {best['max_length']} å­—ç¬¦")
        print(f"  å•é¡Œæ®µè½: {best['problem_count']} å€‹")
        print(f"  æœ€ä½³æ–‡ä»¶: {best['filename']}")
        
        return best
    
    return None

def fair_quality_assessment(transcript_text, srt_analysis, service_name):
    """å…¬å¹³çš„å“è³ªè©•ä¼° - ä¸æ‰£ç°¡ç¹é«”åˆ†æ•¸"""
    print(f"\nğŸ“Š {service_name} å…¬å¹³å“è³ªè©•ä¼°")
    print("=" * 60)
    
    # æ®µè½æ§åˆ¶è©•åˆ† (60%)
    segment_score = 0
    max_length = srt_analysis['max_length']
    problem_count = srt_analysis['problem_count']
    
    if max_length <= 18 and problem_count == 0:
        segment_score = 60  # èˆ‡æœ€ä½³æ–¹æ¡ˆç›¸åŒ
    elif max_length <= 20 and problem_count == 0:
        segment_score = 55
    elif max_length <= 25 and problem_count == 0:
        segment_score = 50
    elif max_length <= 30 and problem_count <= 1:
        segment_score = 40
    else:
        segment_score = 30
    
    # è½‰éŒ„æº–ç¢ºåº¦è©•åˆ† (40%) - ä¸æ‰£ç°¡ç¹é«”åˆ†
    accuracy_score = 0
    
    # æ¨™é»ç¬¦è™Ÿ (15%)
    punctuation_count = transcript_text.count('ï¼Œ') + transcript_text.count('ã€‚') + transcript_text.count('ï¼') + transcript_text.count('ï¼Ÿ')
    punctuation_count += transcript_text.count(',') + transcript_text.count('.') + transcript_text.count('!') + transcript_text.count('?')
    accuracy_score += min(punctuation_count * 1.5, 15)
    
    # å°ˆæ¥­è¡“èªè­˜åˆ¥ (25%) - ä¸å€åˆ†ç°¡ç¹é«”
    terms_found = 0
    if 'å°ç©é›»' in transcript_text or 'å°ç§¯ç”µ' in transcript_text:
        terms_found += 1
    if 'NVIDIA' in transcript_text or 'è¼é”' in transcript_text or 'è¾‰è¾¾' in transcript_text:
        terms_found += 1
    if 'ç´æ–¯é”å…‹' in transcript_text or 'é‚£æ–¯è¾¾å…‹' in transcript_text:
        terms_found += 1
    if 'æ¯”ç‰¹å¹£' in transcript_text or 'æ¯”ç‰¹å¸' in transcript_text:
        terms_found += 1
    
    accuracy_score += terms_found * 6.25  # æ¯å€‹è¡“èª6.25åˆ†
    
    total_score = segment_score + accuracy_score
    
    print(f"ğŸ“ˆ å…¬å¹³è©•åˆ†:")
    print(f"  æ®µè½æ§åˆ¶: {segment_score}/60 (æœ€é•· {max_length} å­—ç¬¦)")
    print(f"  è½‰éŒ„æº–ç¢ºåº¦: {accuracy_score:.1f}/40")
    print(f"  - æ¨™é»ç¬¦è™Ÿ: {punctuation_count} å€‹")
    print(f"  - å°ˆæ¥­è¡“èª: {terms_found}/4 å€‹ (ä¸å€åˆ†ç°¡ç¹é«”)")
    print(f"  ğŸ“Š ç¸½è©•åˆ†: {total_score:.1f}/100")
    
    return {
        'total_score': total_score,
        'segment_score': segment_score,
        'accuracy_score': accuracy_score,
        'punctuation_count': punctuation_count,
        'terms_found': terms_found
    }

def main():
    """å…¬å¹³æ¯”è¼ƒæ¸¬è©¦"""
    print("ğŸ¯ å…¬å¹³æ¯”è¼ƒæ¸¬è©¦ - é‡æ–°è©•ä¼° ElevenLabs å’Œ AssemblyAI")
    print("=" * 80)
    print("ä¿®æ­£ï¼šç°¡ç¹é«”ä¸ä½œç‚ºæ‰£åˆ†é …ï¼Œå°ˆæ³¨æ–¼å¯¦éš›è½‰éŒ„èƒ½åŠ›")
    print("=" * 80)
    
    results = []
    
    # 1. åˆ†æ ElevenLabs çµæœ
    print(f"\nğŸ“Š åˆ†æ ElevenLabs Scribe V1 çµæœ")
    try:
        with open("elevenlabs_scribe_v1_result.json", "r", encoding="utf-8") as f:
            elevenlabs_data = json.load(f)
        
        transcript_text = elevenlabs_data['text']
        words = elevenlabs_data['words']
        
        print(f"ğŸ“ ElevenLabs è½‰éŒ„æ–‡å­—: {transcript_text}")
        
        # æ¸¬è©¦è©å½™ç´šèƒ½åŠ›
        best_word_level = test_word_level_capability(words, "ElevenLabs Scribe", "elevenlabs")
        
        if best_word_level:
            # å…¬å¹³è©•ä¼°
            fair_analysis = fair_quality_assessment(transcript_text, best_word_level, "ElevenLabs Scribe")
            results.append(('ElevenLabs Scribe', fair_analysis, best_word_level))
        
    except Exception as e:
        print(f"âŒ ElevenLabs åˆ†æå¤±æ•—: {str(e)}")
    
    # 2. åˆ†æ AssemblyAI çµæœ
    print(f"\nğŸ“Š åˆ†æ AssemblyAI Universal-1 çµæœ")
    try:
        with open("assemblyai_transcript.txt", "r", encoding="utf-8") as f:
            assemblyai_text = f.read()
        
        # æª¢æŸ¥æ˜¯å¦æœ‰è©å½™ç´šè³‡æ–™
        try:
            with open("assemblyai_chinese_result.json", "r", encoding="utf-8") as f:
                assemblyai_data = json.load(f)
            
            print(f"ğŸ“ AssemblyAI è½‰éŒ„æ–‡å­—: {assemblyai_text}")
            
            # æª¢æŸ¥è©å½™ç´šæ”¯æ´
            if 'words' in assemblyai_data and assemblyai_data['words']:
                words = assemblyai_data['words']
                print(f"âœ… AssemblyAI æ”¯æ´è©å½™ç´šæ™‚é–“æˆ³è¨˜: {len(words)} å€‹è©å½™")
                
                # æ¸¬è©¦è©å½™ç´šèƒ½åŠ›
                best_word_level = test_word_level_capability(words, "AssemblyAI Universal-1", "assemblyai")
                
                if best_word_level:
                    # å…¬å¹³è©•ä¼°
                    fair_analysis = fair_quality_assessment(assemblyai_text, best_word_level, "AssemblyAI Universal-1")
                    results.append(('AssemblyAI Universal-1', fair_analysis, best_word_level))
            else:
                print(f"âŒ AssemblyAI æ²’æœ‰è©å½™ç´šæ™‚é–“æˆ³è¨˜")
                
                # ä½¿ç”¨ç¾æœ‰ SRT åˆ†æ
                with open("assemblyai_custom.srt", "r", encoding="utf-8") as f:
                    srt_content = f.read()
                
                # è§£ææ®µè½é•·åº¦
                lines = srt_content.strip().split('\n')
                lengths = []
                i = 0
                while i < len(lines):
                    if lines[i].strip().isdigit():
                        if i + 2 < len(lines):
                            text_lines = []
                            i += 2
                            while i < len(lines) and lines[i].strip():
                                text_lines.append(lines[i])
                                i += 1
                            
                            if text_lines:
                                text = ' '.join(text_lines).strip()
                                lengths.append(len(text))
                    i += 1
                
                if lengths:
                    srt_analysis = {
                        'max_length': max(lengths),
                        'problem_count': sum(1 for l in lengths if l > 30),
                        'segment_count': len(lengths)
                    }
                    
                    fair_analysis = fair_quality_assessment(assemblyai_text, srt_analysis, "AssemblyAI Universal-1")
                    results.append(('AssemblyAI Universal-1 (æ®µè½ç´š)', fair_analysis, srt_analysis))
        
        except Exception as e:
            print(f"âš ï¸ ç„¡æ³•è®€å– AssemblyAI è©³ç´°çµæœ: {str(e)}")
    
    except Exception as e:
        print(f"âŒ AssemblyAI åˆ†æå¤±æ•—: {str(e)}")
    
    # 3. æœ€çµ‚å…¬å¹³æ¯”è¼ƒ
    print(f"\n" + "=" * 80)
    print(f"ğŸ† å…¬å¹³æ¯”è¼ƒçµæœ (ä¸æ‰£ç°¡ç¹é«”åˆ†æ•¸)")
    print("=" * 80)
    
    # ç¾æœ‰æœ€ä½³æ–¹æ¡ˆé‡æ–°è©•åˆ† (å»é™¤ç°¡ç¹é«”å„ªå‹¢)
    groq_best_score = 50 + 45.2  # æ®µè½æ§åˆ¶50 + è½‰éŒ„å“è³ª45.2 (åŒ…å«ç¹é«”ä¸­æ–‡20åˆ†)
    groq_fair_score = 50 + (45.2 - 20) + 20  # å»é™¤ç¹é«”ä¸­æ–‡å„ªå‹¢ï¼Œä½†ä¿ç•™å…¶ä»–å“è³ª
    
    print(f"ğŸ“Š é‡æ–°è©•åˆ†çš„æœ€ä½³æ–¹æ¡ˆ:")
    print(f"  Groq + Prompt + è©å½™ç´š (å…¬å¹³è©•åˆ†): {groq_fair_score:.1f}/100")
    print(f"  - æ®µè½æ§åˆ¶: 50/60 (18å­—ç¬¦)")
    print(f"  - è½‰éŒ„æº–ç¢ºåº¦: {groq_fair_score - 50:.1f}/40 (æ¨™é»ç¬¦è™Ÿ+å°ˆæ¥­è¡“èª)")
    
    if results:
        print(f"\nğŸ“ˆ æ–°æ¸¬è©¦æœå‹™ (å…¬å¹³è©•åˆ†):")
        
        # æŒ‰è©•åˆ†æ’åº
        results.sort(key=lambda x: x[1]['total_score'], reverse=True)
        
        for i, (service_name, analysis, details) in enumerate(results, 1):
            print(f"\n  {i}. {service_name}: {analysis['total_score']:.1f}/100")
            print(f"     æ®µè½æ§åˆ¶: {analysis['segment_score']}/60")
            print(f"     è½‰éŒ„æº–ç¢ºåº¦: {analysis['accuracy_score']:.1f}/40")
            print(f"     æ¨™é»ç¬¦è™Ÿ: {analysis['punctuation_count']} å€‹")
            print(f"     å°ˆæ¥­è¡“èª: {analysis['terms_found']}/4 å€‹")
            
            if 'max_length' in details:
                print(f"     æœ€é•·æ®µè½: {details['max_length']} å­—ç¬¦")
            
            # èˆ‡æœ€ä½³æ–¹æ¡ˆæ¯”è¼ƒ
            score_diff = analysis['total_score'] - groq_fair_score
            if score_diff > 5:
                print(f"     ğŸ‰ æ¯”æœ€ä½³æ–¹æ¡ˆå¥½ {score_diff:.1f} åˆ†ï¼")
            elif score_diff > 0:
                print(f"     âœ… æ¯”æœ€ä½³æ–¹æ¡ˆç•¥å¥½ (+{score_diff:.1f})")
            elif score_diff > -10:
                print(f"     âš ï¸ æ¥è¿‘æœ€ä½³æ–¹æ¡ˆ ({score_diff:.1f})")
            else:
                print(f"     âŒ ä¸å¦‚æœ€ä½³æ–¹æ¡ˆ ({score_diff:.1f})")
        
        # æœ€çµ‚çµè«–
        best_new = results[0]
        if best_new[1]['total_score'] > groq_fair_score:
            print(f"\nğŸ‰ ç™¼ç¾æ›´å¥½çš„æ–¹æ¡ˆï¼")
            print(f"ğŸ† æ–°çš„æœ€ä½³æ–¹æ¡ˆ: {best_new[0]} ({best_new[1]['total_score']:.1f}åˆ†)")
        else:
            print(f"\nğŸ“Š çµè«–: Groq æœ€ä½³æ–¹æ¡ˆä»ç„¶é ˜å…ˆ")
            print(f"   æœ€æ¥è¿‘çš„æ˜¯ {best_new[0]} ({best_new[1]['total_score']:.1f}åˆ†)")
    
    print(f"\nğŸ‰ å…¬å¹³æ¯”è¼ƒæ¸¬è©¦å®Œæˆï¼")
    return results

if __name__ == "__main__":
    main()
