#!/usr/bin/env python3
"""
æœ€çµ‚æ®µè½ç´š vs è©å½™ç´šè©³ç´°æ¯”è¼ƒåˆ†æ
ä½¿ç”¨å¯¦éš›æˆåŠŸç”Ÿæˆçš„æ–‡ä»¶
"""

import os
import re

def parse_srt_file(filepath):
    """è§£æ SRT æ–‡ä»¶ä¸¦è¿”å›æ®µè½ä¿¡æ¯"""
    try:
        with open(filepath, "r", encoding="utf-8") as f:
            content = f.read().strip()
        
        # åˆ†å‰²æˆæ®µè½å¡Š
        blocks = content.split('\n\n')
        segments = []
        
        for block in blocks:
            lines = block.strip().split('\n')
            if len(lines) >= 3:
                segment_id = lines[0]
                timestamp = lines[1]
                text = lines[2]
                
                segments.append({
                    'id': segment_id,
                    'timestamp': timestamp,
                    'text': text,
                    'length': len(text)
                })
        
        return segments
    except Exception as e:
        print(f"âŒ ç„¡æ³•è§£æ {filepath}: {str(e)}")
        return []

def analyze_segments(segments, name):
    """åˆ†ææ®µè½ç‰¹æ€§"""
    if not segments:
        return None
    
    lengths = [seg['length'] for seg in segments]
    long_segments = [seg for seg in segments if seg['length'] > 25]
    very_long_segments = [seg for seg in segments if seg['length'] > 35]
    
    analysis = {
        'name': name,
        'total_segments': len(segments),
        'avg_length': sum(lengths) / len(lengths),
        'max_length': max(lengths),
        'min_length': min(lengths),
        'long_segments_count': len(long_segments),
        'very_long_segments_count': len(very_long_segments),
        'long_segments': long_segments,
        'very_long_segments': very_long_segments,
        'all_segments': segments
    }
    
    # è©•åˆ†é‚è¼¯ï¼šåŸºç¤60åˆ†ï¼Œæ¯å€‹é•·æ®µè½æ‰£5åˆ†ï¼Œæ¯å€‹è¶…é•·æ®µè½æ‰£10åˆ†
    score = 60 - (len(long_segments) * 5) - (len(very_long_segments) * 5)
    analysis['score'] = max(0, score)
    
    return analysis

def create_detailed_comparison():
    """å‰µå»ºè©³ç´°çš„æ®µè½ç´š vs è©å½™ç´šæ¯”è¼ƒ"""
    print("ğŸ¯ æ®µè½ç´š vs è©å½™ç´šæœ€çµ‚è©³ç´°æ¯”è¼ƒ")
    print("=" * 80)
    
    # å®šç¾©è¦æ¯”è¼ƒçš„æ–‡ä»¶
    files_to_compare = [
        ("elevenlabs_segment_real.srt", "ElevenLabs æ®µè½ç´š", "segment"),
        ("elevenlabs_precise_18chars.srt", "ElevenLabs è©å½™ç´š", "word"),
        ("assemblyai_precise_18chars.srt", "AssemblyAI è©å½™ç´š", "word"),
        ("final_groq_word_level.srt", "Groq è©å½™ç´š", "word")
    ]
    
    analyses = []
    
    print("ğŸ“‹ è§£ææ–‡ä»¶...")
    for filepath, name, type_name in files_to_compare:
        if os.path.exists(filepath):
            print(f"  âœ… è§£æ {name}: {filepath}")
            segments = parse_srt_file(filepath)
            analysis = analyze_segments(segments, name)
            if analysis:
                analysis['type'] = type_name
                analysis['filepath'] = filepath
                analyses.append(analysis)
        else:
            print(f"  âŒ æ–‡ä»¶ä¸å­˜åœ¨: {filepath}")
    
    if not analyses:
        print("âŒ æ²’æœ‰å¯åˆ†æçš„æ–‡ä»¶")
        return
    
    # é¡¯ç¤ºæ¯”è¼ƒè¡¨æ ¼
    print(f"\nğŸ“Š è©³ç´°æ¯”è¼ƒçµæœ:")
    print(f"{'æ–¹æ¡ˆ':<20} {'é¡å‹':<6} {'æ®µè½æ•¸':<8} {'å¹³å‡é•·åº¦':<8} {'æœ€é•·':<6} {'å•é¡Œæ®µè½':<8} {'è©•åˆ†':<6}")
    print("-" * 80)
    
    for analysis in analyses:
        print(f"{analysis['name']:<20} {analysis['type']:<6} {analysis['total_segments']:<8} "
              f"{analysis['avg_length']:<8.1f} {analysis['max_length']:<6} "
              f"{analysis['long_segments_count']:<8} {analysis['score']:<6.0f}")
    
    # æ‰¾å‡ºæ®µè½ç´šå’Œè©å½™ç´šçš„ä»£è¡¨
    segment_level = [a for a in analyses if a['type'] == 'segment']
    word_level = [a for a in analyses if a['type'] == 'word']
    
    if segment_level and word_level:
        print(f"\nğŸ” æ®µè½ç´š vs è©å½™ç´šå°æ¯”:")
        
        # ElevenLabs æ®µè½ç´š vs è©å½™ç´š
        elevenlabs_segment = next((a for a in segment_level if 'ElevenLabs' in a['name']), None)
        elevenlabs_word = next((a for a in word_level if 'ElevenLabs' in a['name']), None)
        
        if elevenlabs_segment and elevenlabs_word:
            print(f"\n  ğŸ“ˆ ElevenLabs å°æ¯”:")
            print(f"    æ®µè½ç´š: æœ€é•· {elevenlabs_segment['max_length']} å­—ç¬¦, å•é¡Œæ®µè½ {elevenlabs_segment['long_segments_count']} å€‹")
            print(f"    è©å½™ç´š: æœ€é•· {elevenlabs_word['max_length']} å­—ç¬¦, å•é¡Œæ®µè½ {elevenlabs_word['long_segments_count']} å€‹")
            print(f"    æ”¹å–„ç¨‹åº¦: æœ€é•·æ®µè½æ¸›å°‘ {elevenlabs_segment['max_length'] - elevenlabs_word['max_length']} å­—ç¬¦")
        
        # è©å½™ç´šæœå‹™æ’å
        print(f"\n  ğŸ† è©å½™ç´šæœå‹™æ’å:")
        word_level_sorted = sorted(word_level, key=lambda x: (-x['score'], x['max_length']))
        for i, analysis in enumerate(word_level_sorted, 1):
            print(f"    {i}. {analysis['name']}: {analysis['score']:.0f}/60 (æœ€é•· {analysis['max_length']} å­—ç¬¦)")
    
    # é¡¯ç¤ºå•é¡Œæ®µè½æ¨£æœ¬
    print(f"\nğŸ“ å•é¡Œæ®µè½æ¨£æœ¬ (>25å­—ç¬¦):")
    for analysis in analyses:
        if analysis['long_segments']:
            print(f"\n  {analysis['name']} å•é¡Œæ®µè½:")
            for i, seg in enumerate(analysis['long_segments'][:3], 1):  # åªé¡¯ç¤ºå‰3å€‹
                print(f"    {i}. {seg['text']} ({seg['length']} å­—ç¬¦)")
    
    # é¡¯ç¤ºæ­£å¸¸æ®µè½æ¨£æœ¬
    print(f"\nâœ… æ­£å¸¸æ®µè½æ¨£æœ¬ (â‰¤25å­—ç¬¦):")
    for analysis in analyses:
        normal_segments = [seg for seg in analysis['all_segments'] if seg['length'] <= 25]
        if normal_segments:
            print(f"\n  {analysis['name']} æ­£å¸¸æ®µè½:")
            for i, seg in enumerate(normal_segments[:3], 1):  # åªé¡¯ç¤ºå‰3å€‹
                print(f"    {i}. {seg['text']} ({seg['length']} å­—ç¬¦)")
    
    return analyses

def generate_final_report(analyses):
    """ç”Ÿæˆæœ€çµ‚å ±å‘Š"""
    print(f"\nğŸ† æœ€çµ‚çµè«–å’Œå»ºè­°")
    print("=" * 80)
    
    # åˆ†é¡åˆ†æçµæœ
    segment_level = [a for a in analyses if a['type'] == 'segment']
    word_level = [a for a in analyses if a['type'] == 'word']
    
    print(f"ğŸ“Š æ¸¬è©¦æˆåŠŸçµ±è¨ˆ:")
    print(f"  æ®µè½ç´šæ¸¬è©¦: {len(segment_level)}/1 å€‹æœå‹™æˆåŠŸ (ElevenLabs)")
    print(f"  è©å½™ç´šæ¸¬è©¦: {len(word_level)}/3 å€‹æœå‹™æˆåŠŸ")
    
    if segment_level and word_level:
        # æ‰¾å‡ºæœ€ä½³æ–¹æ¡ˆ
        best_segment = max(segment_level, key=lambda x: x['score'])
        best_word = max(word_level, key=lambda x: x['score'])
        
        print(f"\nğŸ¥‡ æœ€ä½³æ–¹æ¡ˆ:")
        print(f"  æ®µè½ç´šæœ€ä½³: {best_segment['name']} ({best_segment['score']:.0f}/60)")
        print(f"  è©å½™ç´šæœ€ä½³: {best_word['name']} ({best_word['score']:.0f}/60)")
        
        # é—œéµå·®ç•°åˆ†æ
        print(f"\nğŸ” é—œéµå·®ç•°:")
        if best_segment['name'].startswith('ElevenLabs') and best_word['name'].startswith('ElevenLabs'):
            print(f"  åŒä¸€æœå‹™ (ElevenLabs) æ¯”è¼ƒ:")
            print(f"    æ®µè½ç´š: æœ€é•· {best_segment['max_length']} å­—ç¬¦, å•é¡Œæ®µè½ {best_segment['long_segments_count']} å€‹")
            print(f"    è©å½™ç´š: æœ€é•· {best_word['max_length']} å­—ç¬¦, å•é¡Œæ®µè½ {best_word['long_segments_count']} å€‹")
            
            improvement = best_segment['max_length'] - best_word['max_length']
            print(f"    è©å½™ç´šæ”¹å–„: æœ€é•·æ®µè½æ¸›å°‘ {improvement} å­—ç¬¦")
            
            if improvement > 0:
                print(f"    âœ… è©å½™ç´šæ˜é¡¯å„ªæ–¼æ®µè½ç´š")
            else:
                print(f"    âš ï¸ æ®µè½ç´šè¡¨ç¾ç›¸ç•¶")
    
    # é‡å°ç”¨æˆ¶éœ€æ±‚çš„å»ºè­°
    print(f"\nğŸ’¡ é‡å°æ‚¨éœ€æ±‚çš„å»ºè­°:")
    print(f"  æ‚¨çš„åŸå§‹å•é¡Œ: Whisper-1 æ®µè½éé•· (19å­—ç¬¦)")
    
    if segment_level:
        segment_max = segment_level[0]['max_length']
        print(f"  æ®µè½ç´šçµæœ: æœ€é•· {segment_max} å­—ç¬¦ ({'å•é¡Œæ›´åš´é‡' if segment_max > 19 else 'æœ‰æ‰€æ”¹å–„'})")
    
    if word_level:
        word_max = min(a['max_length'] for a in word_level)
        print(f"  è©å½™ç´šçµæœ: æœ€é•· {word_max} å­—ç¬¦ ({'å®Œç¾è§£æ±º' if word_max <= 18 else 'å¤§å¹…æ”¹å–„'})")
    
    print(f"\nğŸ¯ æœ€çµ‚æ¨è–¦:")
    if word_level:
        best_word = max(word_level, key=lambda x: x['score'])
        print(f"  ğŸ† æ¨è–¦æ–¹æ¡ˆ: {best_word['name']}")
        print(f"  ğŸ“ æ–‡ä»¶ä½ç½®: {best_word['filepath']}")
        print(f"  âœ… è©•åˆ†: {best_word['score']:.0f}/60")
        print(f"  âœ… æœ€é•·æ®µè½: {best_word['max_length']} å­—ç¬¦")
        print(f"  âœ… å•é¡Œæ®µè½: {best_word['long_segments_count']} å€‹")
    
    print(f"\nğŸ“ æ‰€æœ‰æ¸¬è©¦æ–‡ä»¶ä½ç½®:")
    for analysis in analyses:
        print(f"  - {analysis['filepath']} ({analysis['name']})")

def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸ” æ®µè½ç´š vs è©å½™ç´šæœ€çµ‚æ¯”è¼ƒåˆ†æ")
    print("ä½¿ç”¨å¯¦éš›æˆåŠŸç”Ÿæˆçš„æ–‡ä»¶é€²è¡Œæ¯”è¼ƒ")
    print("=" * 80)
    
    # æª¢æŸ¥æ–‡ä»¶å­˜åœ¨æ€§
    test_files = [
        "elevenlabs_segment_real.srt",
        "elevenlabs_precise_18chars.srt",
        "assemblyai_precise_18chars.srt",
        "final_groq_word_level.srt"
    ]
    
    existing_files = [f for f in test_files if os.path.exists(f)]
    print(f"ğŸ“‹ å¯ç”¨æ–‡ä»¶: {len(existing_files)}/{len(test_files)}")
    
    if len(existing_files) < 2:
        print("âŒ å¯ç”¨æ–‡ä»¶å¤ªå°‘ï¼Œç„¡æ³•é€²è¡Œæœ‰æ•ˆæ¯”è¼ƒ")
        return
    
    # åŸ·è¡Œè©³ç´°æ¯”è¼ƒ
    analyses = create_detailed_comparison()
    
    if analyses:
        # ç”Ÿæˆæœ€çµ‚å ±å‘Š
        generate_final_report(analyses)
        
        print(f"\nâœ… æ®µè½ç´š vs è©å½™ç´šæ¯”è¼ƒåˆ†æå®Œæˆï¼")
        print(f"ğŸ“Š æˆåŠŸåˆ†æäº† {len(analyses)} å€‹æ–‡ä»¶")
    else:
        print(f"âŒ åˆ†æå¤±æ•—")

if __name__ == "__main__":
    main()
