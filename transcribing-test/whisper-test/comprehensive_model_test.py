#!/usr/bin/env python3
"""
å…¨é¢æ¸¬è©¦æ‰€æœ‰èªéŸ³è½‰æ–‡å­—æ¨¡å‹çš„ Prompt åŠŸèƒ½å’Œå¯¦éš›æ•ˆæœ
åŒ…æ‹¬å¯¦éš›åˆ†æ SRT å…§å®¹å“è³ªå’Œåˆç†æ€§
"""

import os
from dotenv import load_dotenv
import time
import json
from openai import OpenAI

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

def format_srt_time(seconds):
    """å°‡ç§’æ•¸è½‰æ›ç‚º SRT æ™‚é–“æ ¼å¼"""
    hours = int(seconds // 3600)
    minutes = int((seconds % 3600) // 60)
    secs = int(seconds % 60)
    millisecs = int((seconds % 1) * 1000)
    return f"{hours:02d}:{minutes:02d}:{secs:02d},{millisecs:03d}"

def create_srt_from_segments(segments):
    """å¾æ®µè½è³‡è¨Šå‰µå»º SRT"""
    srt_content = []
    
    for i, segment in enumerate(segments, 1):
        start_time = format_srt_time(segment.start)
        end_time = format_srt_time(segment.end)
        text = segment.text.strip()
        
        srt_content.append(f"{i}")
        srt_content.append(f"{start_time} --> {end_time}")
        srt_content.append(text)
        srt_content.append("")
    
    return "\n".join(srt_content)

def analyze_srt_content_quality(srt_content, model_name):
    """å¯¦éš›åˆ†æ SRT å…§å®¹å“è³ªå’Œåˆç†æ€§"""
    print(f"\nğŸ” {model_name} SRT å…§å®¹å“è³ªåˆ†æ")
    print("=" * 50)
    
    lines = srt_content.strip().split('\n')
    segments = []
    current_segment = {"id": None, "time": None, "text": ""}
    
    for line in lines:
        if line.strip().isdigit():
            if current_segment["text"]:
                segments.append(current_segment.copy())
            current_segment = {"id": int(line), "time": None, "text": ""}
        elif "-->" in line:
            current_segment["time"] = line.strip()
        elif line.strip():
            current_segment["text"] += line + " "
    
    if current_segment["text"]:
        segments.append(current_segment)
    
    # åˆ†æçµ±è¨ˆæ•¸æ“š
    segment_lengths = [len(seg["text"].strip()) for seg in segments]
    
    print(f"ğŸ“Š åŸºæœ¬çµ±è¨ˆ:")
    print(f"  ç¸½æ®µè½æ•¸: {len(segments)}")
    print(f"  å¹³å‡æ®µè½é•·åº¦: {sum(segment_lengths) / len(segment_lengths):.1f} å­—ç¬¦")
    print(f"  æœ€é•·æ®µè½: {max(segment_lengths)} å­—ç¬¦")
    print(f"  æœ€çŸ­æ®µè½: {min(segment_lengths)} å­—ç¬¦")
    
    # åˆ†ææ®µè½é•·åº¦åˆ†ä½ˆ
    short_segments = [s for s in segment_lengths if s <= 10]
    medium_segments = [s for s in segment_lengths if 10 < s <= 25]
    long_segments = [s for s in segment_lengths if s > 25]
    
    print(f"ğŸ“ˆ æ®µè½é•·åº¦åˆ†ä½ˆ:")
    print(f"  çŸ­æ®µè½ (â‰¤10å­—ç¬¦): {len(short_segments)} å€‹ ({len(short_segments)/len(segments)*100:.1f}%)")
    print(f"  ä¸­ç­‰æ®µè½ (11-25å­—ç¬¦): {len(medium_segments)} å€‹ ({len(medium_segments)/len(segments)*100:.1f}%)")
    print(f"  é•·æ®µè½ (>25å­—ç¬¦): {len(long_segments)} å€‹ ({len(long_segments)/len(segments)*100:.1f}%)")
    
    # åˆ†æå¯¦éš›å…§å®¹å“è³ª
    print(f"\nğŸ“ å…§å®¹å“è³ªåˆ†æ:")
    
    # æª¢æŸ¥å‰ 5 å€‹æ®µè½çš„å¯¦éš›å…§å®¹
    print(f"å‰ 5 å€‹æ®µè½å¯¦éš›å…§å®¹:")
    for i, seg in enumerate(segments[:5]):
        text = seg["text"].strip()
        time_info = seg["time"]
        
        # åˆ†æé€™å€‹æ®µè½çš„åˆç†æ€§
        is_complete = text.endswith(('ã€‚', 'ï¼', 'ï¼Ÿ', '.', '!', '?'))
        has_punctuation = any(p in text for p in ['ï¼Œ', 'ã€‚', 'ï¼', 'ï¼Ÿ', ',', '.', '!', '?'])
        
        print(f"  {i+1}. [{time_info}] ({len(text)}å­—ç¬¦)")
        print(f"     å…§å®¹: {text}")
        print(f"     å®Œæ•´æ€§: {'âœ… å®Œæ•´å¥å­' if is_complete else 'âŒ ä¸å®Œæ•´'}")
        print(f"     æ¨™é»ç¬¦è™Ÿ: {'âœ… æœ‰æ¨™é»' if has_punctuation else 'âŒ ç„¡æ¨™é»'}")
        print()
    
    # æª¢æŸ¥æ˜¯å¦æœ‰ç•°å¸¸é•·çš„æ®µè½
    if long_segments:
        print(f"âš ï¸ ç™¼ç¾ {len(long_segments)} å€‹éé•·æ®µè½:")
        for i, seg in enumerate(segments):
            if len(seg["text"].strip()) > 25:
                text = seg["text"].strip()
                print(f"  æ®µè½ {seg['id']}: {len(text)} å­—ç¬¦")
                print(f"    å…§å®¹: {text[:100]}{'...' if len(text) > 100 else ''}")
    
    return {
        'total_segments': len(segments),
        'avg_length': sum(segment_lengths) / len(segment_lengths),
        'max_length': max(segment_lengths),
        'min_length': min(segment_lengths),
        'short_segments': len(short_segments),
        'medium_segments': len(medium_segments),
        'long_segments': len(long_segments),
        'segments': segments[:5]  # ä¿å­˜å‰ 5 å€‹æ®µè½ç”¨æ–¼æ¯”è¼ƒ
    }

def test_model_with_prompts(client, model, audio_file, prompts_to_test):
    """æ¸¬è©¦æ¨¡å‹çš„ Prompt åŠŸèƒ½"""
    print(f"\nğŸš€ æ¸¬è©¦ {model} çš„ Prompt åŠŸèƒ½")
    print("=" * 60)
    
    results = []
    
    for prompt_name, prompt_text in prompts_to_test:
        print(f"\n--- æ¸¬è©¦ Prompt: {prompt_name} ---")
        print(f"Prompt å…§å®¹: {prompt_text}")
        
        try:
            start_time = time.time()
            
            # æ ¹æ“šæ¨¡å‹é¡å‹èª¿æ•´åƒæ•¸
            params = {
                "model": model,
                "file": open(audio_file, "rb"),
                "language": "zh"
            }
            
            # æ·»åŠ  prompt åƒæ•¸
            if prompt_text:
                params["prompt"] = prompt_text
            
            # æ ¹æ“šæ¨¡å‹æ±ºå®šæ ¼å¼
            if "gpt-4o" in model:
                params["response_format"] = "json"
            elif model == "whisper-1":
                params["response_format"] = "verbose_json"
                params["timestamp_granularities"] = ["segment", "word"]
            
            transcription = client.audio.transcriptions.create(**params)
            
            processing_time = time.time() - start_time
            
            print(f"âœ… æˆåŠŸ - è™•ç†æ™‚é–“: {processing_time:.2f} ç§’")
            
            # ç²å–æ–‡å­—çµæœ
            if hasattr(transcription, 'text'):
                text = transcription.text
            else:
                text = str(transcription)
            
            print(f"ğŸ“ æ–‡å­—é•·åº¦: {len(text)} å­—ç¬¦")
            print(f"æ–‡å­—é è¦½: {text[:200]}...")
            
            # å¦‚æœæœ‰æ®µè½è³‡è¨Šï¼Œå‰µå»º SRT
            srt_content = None
            if hasattr(transcription, 'segments') and transcription.segments:
                print(f"ğŸ“Š æ®µè½æ•¸: {len(transcription.segments)}")
                srt_content = create_srt_from_segments(transcription.segments)
                
                # ä¿å­˜ SRT æ–‡ä»¶
                filename = f"{model.replace('-', '_')}_{prompt_name.replace(' ', '_').lower()}.srt"
                with open(filename, "w", encoding="utf-8") as f:
                    f.write(srt_content)
                print(f"ğŸ’¾ SRT å·²ä¿å­˜: {filename}")
            
            # æª¢æŸ¥è©å½™ç´šæ™‚é–“æˆ³è¨˜
            word_count = 0
            if hasattr(transcription, 'words') and transcription.words:
                word_count = len(transcription.words)
                print(f"ğŸ“ è©å½™ç´šæ™‚é–“æˆ³è¨˜: {word_count} å€‹è©å½™")
            
            results.append({
                'model': model,
                'prompt_name': prompt_name,
                'prompt_text': prompt_text,
                'success': True,
                'processing_time': processing_time,
                'text': text,
                'text_length': len(text),
                'segment_count': len(transcription.segments) if hasattr(transcription, 'segments') and transcription.segments else 0,
                'word_count': word_count,
                'srt_content': srt_content
            })
            
        except Exception as e:
            print(f"âŒ å¤±æ•— - éŒ¯èª¤: {str(e)}")
            results.append({
                'model': model,
                'prompt_name': prompt_name,
                'prompt_text': prompt_text,
                'success': False,
                'error': str(e)
            })
        
        time.sleep(1)  # é¿å… API é™åˆ¶
    
    return results

def test_groq_model(groq_client, audio_file, prompts_to_test):
    """æ¸¬è©¦ Groq Whisper Large v3"""
    print(f"\nğŸš€ æ¸¬è©¦ Groq Whisper Large v3 çš„ Prompt åŠŸèƒ½")
    print("=" * 60)
    
    results = []
    
    for prompt_name, prompt_text in prompts_to_test:
        print(f"\n--- æ¸¬è©¦ Prompt: {prompt_name} ---")
        print(f"Prompt å…§å®¹: {prompt_text}")
        
        try:
            start_time = time.time()
            
            params = {
                "model": "whisper-large-v3",
                "file": open(audio_file, "rb"),
                "response_format": "verbose_json",
                "language": "zh"
            }
            
            # æ·»åŠ  prompt åƒæ•¸
            if prompt_text:
                params["prompt"] = prompt_text
            
            transcription = groq_client.audio.transcriptions.create(**params)
            
            processing_time = time.time() - start_time
            
            print(f"âœ… æˆåŠŸ - è™•ç†æ™‚é–“: {processing_time:.2f} ç§’")
            print(f"ğŸ“ æ–‡å­—é•·åº¦: {len(transcription.text)} å­—ç¬¦")
            print(f"æ–‡å­—é è¦½: {transcription.text[:200]}...")
            
            srt_content = None
            if transcription.segments:
                print(f"ğŸ“Š æ®µè½æ•¸: {len(transcription.segments)}")
                srt_content = create_srt_from_segments(transcription.segments)
                
                # ä¿å­˜ SRT æ–‡ä»¶
                filename = f"groq_whisper_large_v3_{prompt_name.replace(' ', '_').lower()}.srt"
                with open(filename, "w", encoding="utf-8") as f:
                    f.write(srt_content)
                print(f"ğŸ’¾ SRT å·²ä¿å­˜: {filename}")
            
            results.append({
                'model': 'groq-whisper-large-v3',
                'prompt_name': prompt_name,
                'prompt_text': prompt_text,
                'success': True,
                'processing_time': processing_time,
                'text': transcription.text,
                'text_length': len(transcription.text),
                'segment_count': len(transcription.segments) if transcription.segments else 0,
                'word_count': 0,  # Groq ä¸æ”¯æ´è©å½™ç´š
                'srt_content': srt_content
            })
            
        except Exception as e:
            print(f"âŒ å¤±æ•— - éŒ¯èª¤: {str(e)}")
            results.append({
                'model': 'groq-whisper-large-v3',
                'prompt_name': prompt_name,
                'prompt_text': prompt_text,
                'success': False,
                'error': str(e)
            })
        
        time.sleep(1)
    
    return results

def main():
    """ä¸»æ¸¬è©¦å‡½æ•¸"""
    print("ğŸ¯ å…¨é¢èªéŸ³è½‰æ–‡å­—æ¨¡å‹æ¸¬è©¦ - åŒ…å« Prompt åŠŸèƒ½å’Œå“è³ªåˆ†æ")
    print("=" * 80)
    
    # API é‡‘é‘°
    openai_api_key = os.getenv("OPENAI_API_KEY")
    groq_api_key = os.getenv("GROQ_API_KEY")
    
    audio_file = "test_audio.mp3"
    
    if not os.path.exists(audio_file):
        print(f"âŒ éŒ¯èª¤: æ‰¾ä¸åˆ°éŸ³æª” {audio_file}")
        return
    
    # åˆå§‹åŒ–å®¢æˆ¶ç«¯
    openai_client = OpenAI(api_key=openai_api_key)
    groq_client = OpenAI(
        api_key=groq_api_key,
        base_url="https://api.groq.com/openai/v1"
    )
    
    # å®šç¾©è¦æ¸¬è©¦çš„ Prompts
    prompts_to_test = [
        ("ç„¡ Prompt", ""),
        ("è²¡ç¶“æ–°è", "ä»¥ä¸‹æ˜¯é—œæ–¼è‚¡å¸‚å’Œé‡‘èå¸‚å ´çš„è²¡ç¶“æ–°èå ±å°ã€‚åŒ…å«è‚¡ç¥¨ã€åŒ¯ç‡ã€ç¶“æ¿Ÿæ•¸æ“šç­‰å°ˆæ¥­è¡“èªã€‚"),
        ("æ¨™é»ç¬¦è™Ÿå¢å¼·", "è«‹æ­£ç¢ºæ·»åŠ æ¨™é»ç¬¦è™Ÿï¼ŒåŒ…æ‹¬é€—è™Ÿã€å¥è™Ÿã€å•è™Ÿå’Œæ„Ÿå˜†è™Ÿã€‚æ–‡å­—æ‡‰è©²æœ‰é©ç•¶çš„åœé “å’Œèªèª¿æ¨™è¨˜ã€‚"),
        ("å°ˆæ¥­è¡“èª", "é€™æ˜¯ä¸€æ®µåŒ…å« NVIDIAã€å°ç©é›»ã€ADRã€é‚£æ–¯é”å…‹ã€æ¯”ç‰¹å¹£ç­‰å°ˆæ¥­é‡‘èè¡“èªçš„å…§å®¹ã€‚"),
        ("å®Œæ•´å¥å­", "è«‹å°‡è½‰éŒ„çµæœçµ„ç¹”æˆå®Œæ•´çš„å¥å­ï¼Œæ¯å€‹å¥å­éƒ½æ‡‰è©²æœ‰æ˜ç¢ºçš„é–‹å§‹å’ŒçµæŸã€‚é¿å…éé•·çš„å¥å­ã€‚")
    ]
    
    # æ¸¬è©¦æ‰€æœ‰æ¨¡å‹
    all_results = []
    
    # 1. æ¸¬è©¦ OpenAI whisper-1
    print(f"\nğŸ” ç¬¬ä¸€éƒ¨åˆ†ï¼šOpenAI Whisper-1 æ¸¬è©¦")
    whisper1_results = test_model_with_prompts(openai_client, "whisper-1", audio_file, prompts_to_test)
    all_results.extend(whisper1_results)
    
    # 2. æ¸¬è©¦ OpenAI æ–°æ¨¡å‹
    print(f"\nğŸ” ç¬¬äºŒéƒ¨åˆ†ï¼šOpenAI æ–°æ¨¡å‹æ¸¬è©¦")
    for model in ["gpt-4o-transcribe", "gpt-4o-mini-transcribe"]:
        model_results = test_model_with_prompts(openai_client, model, audio_file, prompts_to_test)
        all_results.extend(model_results)
    
    # 3. æ¸¬è©¦ Groq Whisper Large v3
    print(f"\nğŸ” ç¬¬ä¸‰éƒ¨åˆ†ï¼šGroq Whisper Large v3 æ¸¬è©¦")
    groq_results = test_groq_model(groq_client, audio_file, prompts_to_test)
    all_results.extend(groq_results)
    
    # 4. åˆ†ææ‰€æœ‰æˆåŠŸçš„ SRT çµæœ
    print(f"\n" + "=" * 80)
    print(f"ğŸ“Š SRT å…§å®¹å“è³ªæ·±åº¦åˆ†æ")
    print("=" * 80)
    
    srt_analyses = {}
    successful_results = [r for r in all_results if r['success'] and r.get('srt_content')]
    
    for result in successful_results:
        model_prompt = f"{result['model']}_{result['prompt_name']}"
        if result['srt_content']:
            analysis = analyze_srt_content_quality(result['srt_content'], model_prompt)
            srt_analyses[model_prompt] = analysis
    
    # 5. æ¯”è¼ƒåˆ†æ
    if srt_analyses:
        print(f"\n" + "=" * 80)
        print(f"ğŸ† æ¨¡å‹å’Œ Prompt æ•ˆæœæ¯”è¼ƒ")
        print("=" * 80)
        
        print(f"ğŸ“ˆ æ®µè½é•·åº¦æ¯”è¼ƒ:")
        for model_prompt, analysis in srt_analyses.items():
            print(f"  {model_prompt}:")
            print(f"    å¹³å‡é•·åº¦: {analysis['avg_length']:.1f} å­—ç¬¦")
            print(f"    æœ€é•·æ®µè½: {analysis['max_length']} å­—ç¬¦") 
            print(f"    é•·æ®µè½æ¯”ä¾‹: {analysis['long_segments']}/{analysis['total_segments']} ({analysis['long_segments']/analysis['total_segments']*100:.1f}%)")
            print()
        
        # æ‰¾å‡ºæœ€ä½³è¡¨ç¾
        best_avg = min(srt_analyses.items(), key=lambda x: x[1]['avg_length'])
        best_long_ratio = min(srt_analyses.items(), key=lambda x: x[1]['long_segments']/x[1]['total_segments'])
        
        print(f"ğŸ… æœ€ä½³è¡¨ç¾:")
        print(f"  æœ€çŸ­å¹³å‡æ®µè½: {best_avg[0]} ({best_avg[1]['avg_length']:.1f} å­—ç¬¦)")
        print(f"  æœ€å°‘é•·æ®µè½: {best_long_ratio[0]} ({best_long_ratio[1]['long_segments']}/{best_long_ratio[1]['total_segments']} = {best_long_ratio[1]['long_segments']/best_long_ratio[1]['total_segments']*100:.1f}%)")
    
    # 6. ä¿å­˜å®Œæ•´çµæœ
    with open("comprehensive_model_test_results.json", "w", encoding="utf-8") as f:
        json.dump({
            'test_results': all_results,
            'srt_analyses': srt_analyses
        }, f, ensure_ascii=False, indent=2, default=str)
    
    print(f"\nğŸ’¾ å®Œæ•´æ¸¬è©¦çµæœå·²ä¿å­˜åˆ°: comprehensive_model_test_results.json")
    print("ğŸ‰ æ¸¬è©¦å®Œæˆï¼")

if __name__ == "__main__":
    main()
