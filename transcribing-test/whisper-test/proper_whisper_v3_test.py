#!/usr/bin/env python3
"""
æ­£ç¢ºæ¸¬è©¦ Whisper Large v3 vs Whisper-1 çš„å¯¦éš›æ•ˆæœæ¯”è¼ƒ
"""

import os
from dotenv import load_dotenv
import time
from openai import OpenAI

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

def format_srt_time(seconds):
    """å°‡ç§’æ•¸è½‰æ›ç‚º SRT æ™‚é–“æ ¼å¼"""
    hours = int(seconds // 3600)
    minutes = int((seconds % 3600) // 60)
    secs = int(seconds % 60)
    millisecs = int((seconds % 1) * 1000)
    return f"{hours:02d}:{minutes:02d}:{secs:02d},{millisecs:03d}"

def create_srt_from_segments(segments):
    """å¾æ®µè½è³‡è¨Šå‰µå»º SRT"""
    srt_content = []
    
    for i, segment in enumerate(segments, 1):
        start_time = format_srt_time(segment.start)
        end_time = format_srt_time(segment.end)
        text = segment.text.strip()
        
        srt_content.append(f"{i}")
        srt_content.append(f"{start_time} --> {end_time}")
        srt_content.append(text)
        srt_content.append("")
    
    return "\n".join(srt_content)

def analyze_real_srt_quality(srt_content, model_name):
    """å¯¦éš›åˆ†æ SRT å“è³ª - å°ˆæ³¨æ–¼æ‚¨çš„éœ€æ±‚"""
    print(f"\nğŸ” {model_name} - å¯¦éš› SRT å“è³ªåˆ†æ")
    print("=" * 60)
    
    # è§£æ SRT
    lines = srt_content.strip().split('\n')
    segments = []
    
    i = 0
    while i < len(lines):
        if lines[i].strip().isdigit():
            if i + 2 < len(lines):
                segment_id = int(lines[i])
                time_line = lines[i + 1]
                text_lines = []
                i += 2
                while i < len(lines) and lines[i].strip():
                    text_lines.append(lines[i])
                    i += 1
                
                if text_lines:
                    text = ' '.join(text_lines).strip()
                    segments.append({
                        'id': segment_id,
                        'time': time_line,
                        'text': text,
                        'length': len(text)
                    })
        i += 1
    
    if not segments:
        print("âŒ ç„¡æ³•è§£æ SRT")
        return None
    
    lengths = [seg['length'] for seg in segments]
    
    print(f"ğŸ“Š åŸºæœ¬çµ±è¨ˆ:")
    print(f"  ç¸½æ®µè½æ•¸: {len(segments)}")
    print(f"  å¹³å‡é•·åº¦: {sum(lengths)/len(lengths):.1f} å­—ç¬¦")
    print(f"  æœ€é•·æ®µè½: {max(lengths)} å­—ç¬¦")
    print(f"  æœ€çŸ­æ®µè½: {min(lengths)} å­—ç¬¦")
    
    # æŒ‰æ‚¨çš„æ¨™æº–åˆ†é¡
    too_long_40 = [l for l in lengths if l > 40]  # åš´é‡éé•·
    too_long_30 = [l for l in lengths if l > 30]  # éé•·
    ideal = [l for l in lengths if 15 <= l <= 25]  # ç†æƒ³
    too_short = [l for l in lengths if l < 10]  # éçŸ­
    
    print(f"\nğŸ¯ é‡å°å­—å¹•ä½¿ç”¨çš„åˆ†æ:")
    print(f"  ç†æƒ³é•·åº¦ (15-25å­—ç¬¦): {len(ideal)} å€‹ ({len(ideal)/len(segments)*100:.1f}%)")
    print(f"  éé•· (>30å­—ç¬¦): {len(too_long_30)} å€‹ ({len(too_long_30)/len(segments)*100:.1f}%)")
    print(f"  åš´é‡éé•· (>40å­—ç¬¦): {len(too_long_40)} å€‹ ({len(too_long_40)/len(segments)*100:.1f}%)")
    print(f"  éçŸ­ (<10å­—ç¬¦): {len(too_short)} å€‹ ({len(too_short)/len(segments)*100:.1f}%)")
    
    # é¡¯ç¤ºå¯¦éš›å…§å®¹
    print(f"\nğŸ“º å¯¦éš›å­—å¹•å…§å®¹æª¢æŸ¥:")
    for i, seg in enumerate(segments[:8]):
        length_status = ""
        if seg['length'] > 40:
            length_status = "ğŸš¨ åš´é‡éé•·"
        elif seg['length'] > 30:
            length_status = "âš ï¸ éé•·"
        elif 15 <= seg['length'] <= 25:
            length_status = "âœ… ç†æƒ³"
        elif seg['length'] < 10:
            length_status = "ğŸ”¸ éçŸ­"
        else:
            length_status = "ğŸ“ å¯æ¥å—"
        
        print(f"  {seg['id']}. [{seg['time']}] ({seg['length']}å­—ç¬¦) {length_status}")
        print(f"      {seg['text']}")
        print()
    
    # è¨ˆç®—å¯¦ç”¨æ€§è©•åˆ†
    usability_score = 0
    
    # æ²’æœ‰åš´é‡éé•·æ®µè½ (40åˆ†)
    if len(too_long_40) == 0:
        usability_score += 40
    elif len(too_long_40) <= 2:
        usability_score += 20
    
    # ç†æƒ³æ®µè½æ¯”ä¾‹ (30åˆ†)
    usability_score += (len(ideal) / len(segments)) * 30
    
    # éé•·æ®µè½æ§åˆ¶ (20åˆ†)
    if len(too_long_30) == 0:
        usability_score += 20
    elif len(too_long_30) <= 3:
        usability_score += 10
    
    # éçŸ­æ®µè½æ§åˆ¶ (10åˆ†)
    if len(too_short) / len(segments) < 0.2:
        usability_score += 10
    elif len(too_short) / len(segments) < 0.4:
        usability_score += 5
    
    print(f"ğŸ“ˆ å­—å¹•å¯¦ç”¨æ€§è©•åˆ†: {usability_score:.1f}/100")
    
    if usability_score >= 80:
        rating = "ğŸ† å„ªç§€"
    elif usability_score >= 60:
        rating = "âœ… è‰¯å¥½"
    elif usability_score >= 40:
        rating = "âš ï¸ ä¸€èˆ¬"
    else:
        rating = "âŒ ä¸ä½³"
    
    print(f"ğŸ¯ å¯¦ç”¨æ€§è©•ç´š: {rating}")
    
    return {
        'total_segments': len(segments),
        'avg_length': sum(lengths)/len(lengths),
        'max_length': max(lengths),
        'min_length': min(lengths),
        'ideal_count': len(ideal),
        'too_long_30_count': len(too_long_30),
        'too_long_40_count': len(too_long_40),
        'too_short_count': len(too_short),
        'usability_score': usability_score,
        'rating': rating,
        'ideal_ratio': len(ideal)/len(segments),
        'too_long_ratio': len(too_long_30)/len(segments)
    }

def main():
    """é‡æ–°æ­£ç¢ºæ¸¬è©¦ Whisper Large v3 vs Whisper-1"""
    print("ğŸ¯ é‡æ–°å¯¦æ¸¬ï¼šå°‹æ‰¾æ¯” Whisper-1 æ›´å¥½çš„æ¨¡å‹")
    print("=" * 80)
    
    # API è¨­å®š
    openai_api_key = os.getenv("OPENAI_API_KEY")
    groq_api_key = os.getenv("GROQ_API_KEY")
    
    audio_file = "test_audio.mp3"
    
    if not os.path.exists(audio_file):
        print(f"âŒ éŒ¯èª¤: æ‰¾ä¸åˆ°éŸ³æª” {audio_file}")
        return
    
    # åˆå§‹åŒ–å®¢æˆ¶ç«¯
    openai_client = OpenAI(api_key=openai_api_key)
    groq_client = OpenAI(
        api_key=groq_api_key,
        base_url="https://api.groq.com/openai/v1"
    )
    
    results = []
    
    # 1. æ¸¬è©¦ OpenAI Whisper-1 åŸºæº–
    print(f"\nğŸ“Š æ¸¬è©¦ 1: OpenAI Whisper-1 (åŸºæº–)")
    try:
        start_time = time.time()
        
        with open(audio_file, "rb") as f:
            transcription = openai_client.audio.transcriptions.create(
                model="whisper-1",
                file=f,
                response_format="srt",
                language="zh"
            )
        
        processing_time = time.time() - start_time
        print(f"âœ… æˆåŠŸ - è™•ç†æ™‚é–“: {processing_time:.2f} ç§’")
        
        # ä¿å­˜çµæœ
        with open("whisper1_baseline_final.srt", "w", encoding="utf-8") as f:
            f.write(transcription)
        
        # åˆ†æå“è³ª
        analysis = analyze_real_srt_quality(transcription, "OpenAI Whisper-1 åŸºæº–")
        
        results.append({
            'model': 'OpenAI Whisper-1',
            'config': 'åŸºæº–',
            'processing_time': processing_time,
            'analysis': analysis,
            'srt_content': transcription
        })
        
    except Exception as e:
        print(f"âŒ å¤±æ•—: {str(e)}")
    
    # 2. æ¸¬è©¦ Groq Whisper Large v3 åŸºæº–
    print(f"\nğŸ“Š æ¸¬è©¦ 2: Groq Whisper Large v3 (åŸºæº–)")
    try:
        start_time = time.time()
        
        with open(audio_file, "rb") as f:
            transcription = groq_client.audio.transcriptions.create(
                model="whisper-large-v3",
                file=f,
                response_format="verbose_json",
                language="zh"
            )
        
        processing_time = time.time() - start_time
        print(f"âœ… æˆåŠŸ - è™•ç†æ™‚é–“: {processing_time:.2f} ç§’")
        print(f"ğŸ“Š æ®µè½æ•¸: {len(transcription.segments)}")
        
        # å‰µå»º SRT
        srt_content = create_srt_from_segments(transcription.segments)
        
        # ä¿å­˜çµæœ
        with open("groq_large_v3_baseline_final.srt", "w", encoding="utf-8") as f:
            f.write(srt_content)
        
        # åˆ†æå“è³ª
        analysis = analyze_real_srt_quality(srt_content, "Groq Whisper Large v3 åŸºæº–")
        
        results.append({
            'model': 'Groq Whisper Large v3',
            'config': 'åŸºæº–',
            'processing_time': processing_time,
            'analysis': analysis,
            'srt_content': srt_content
        })
        
    except Exception as e:
        print(f"âŒ å¤±æ•—: {str(e)}")
    
    # 3. æ¸¬è©¦ Groq Whisper Large v3 + å„ªåŒ– Prompt
    print(f"\nğŸ“Š æ¸¬è©¦ 3: Groq Whisper Large v3 + å„ªåŒ– Prompt")
    
    optimized_prompt = """è²¡ç¶“æ–°èè½‰éŒ„ã€‚è¦æ±‚ï¼š
1. æ®µè½é•·åº¦ 15-25 å­—ç¬¦
2. è‡ªç„¶åœé “åˆ†æ®µ
3. ä¿æŒèªç¾©å®Œæ•´
4. æ­£ç¢ºè­˜åˆ¥ï¼šå°ç©é›»ã€NVIDIAã€ADRã€é‚£æ–¯é”å…‹ã€æ¯”ç‰¹å¹£
5. é©ç•¶æ¨™é»ç¬¦è™Ÿ"""
    
    try:
        start_time = time.time()
        
        with open(audio_file, "rb") as f:
            transcription = groq_client.audio.transcriptions.create(
                model="whisper-large-v3",
                file=f,
                response_format="verbose_json",
                prompt=optimized_prompt,
                language="zh"
            )
        
        processing_time = time.time() - start_time
        print(f"âœ… æˆåŠŸ - è™•ç†æ™‚é–“: {processing_time:.2f} ç§’")
        print(f"ğŸ“Š æ®µè½æ•¸: {len(transcription.segments)}")
        
        # å‰µå»º SRT
        srt_content = create_srt_from_segments(transcription.segments)
        
        # ä¿å­˜çµæœ
        with open("groq_large_v3_optimized_final.srt", "w", encoding="utf-8") as f:
            f.write(srt_content)
        
        # åˆ†æå“è³ª
        analysis = analyze_real_srt_quality(srt_content, "Groq Whisper Large v3 å„ªåŒ–")
        
        results.append({
            'model': 'Groq Whisper Large v3',
            'config': 'å„ªåŒ– Prompt',
            'processing_time': processing_time,
            'analysis': analysis,
            'srt_content': srt_content
        })
        
    except Exception as e:
        print(f"âŒ å¤±æ•—: {str(e)}")
    
    # 4. æœ€çµ‚æ¯”è¼ƒ
    print(f"\n" + "=" * 80)
    print(f"ğŸ† å¯¦æ¸¬çµæœï¼šå“ªå€‹æ¨¡å‹çœŸçš„æ¯” Whisper-1 æ›´å¥½ï¼Ÿ")
    print("=" * 80)
    
    if len(results) >= 2:
        # æ‰¾åˆ° Whisper-1 åŸºæº–
        whisper1_result = next((r for r in results if 'Whisper-1' in r['model']), None)
        other_results = [r for r in results if 'Whisper-1' not in r['model']]
        
        if whisper1_result:
            whisper1_score = whisper1_result['analysis']['usability_score']
            whisper1_time = whisper1_result['processing_time']
            
            print(f"ğŸ“Š Whisper-1 åŸºæº–è¡¨ç¾:")
            print(f"  å¯¦ç”¨æ€§è©•åˆ†: {whisper1_score:.1f}/100")
            print(f"  è™•ç†é€Ÿåº¦: {whisper1_time:.2f} ç§’")
            print(f"  è©•ç´š: {whisper1_result['analysis']['rating']}")
            
            print(f"\nğŸš€ å…¶ä»–æ¨¡å‹ vs Whisper-1 æ¯”è¼ƒ:")
            
            better_models = []
            
            for result in other_results:
                other_score = result['analysis']['usability_score']
                other_time = result['processing_time']
                
                score_diff = other_score - whisper1_score
                speed_improvement = ((whisper1_time - other_time) / whisper1_time) * 100
                
                print(f"\n  {result['model']} ({result['config']}):")
                print(f"    å¯¦ç”¨æ€§è©•åˆ†: {other_score:.1f}/100 ({score_diff:+.1f})")
                print(f"    è™•ç†é€Ÿåº¦: {other_time:.2f} ç§’ ({speed_improvement:+.1f}%)")
                print(f"    è©•ç´š: {result['analysis']['rating']}")
                
                if score_diff > 5 or speed_improvement > 50:
                    print(f"    ğŸ‰ æ¯” Whisper-1 æ›´å¥½ï¼")
                    better_models.append(result)
                elif score_diff > 0:
                    print(f"    âœ… ç•¥å„ªæ–¼ Whisper-1")
                    better_models.append(result)
                else:
                    print(f"    âŒ ä¸å¦‚ Whisper-1")
            
            # ç¸½çµ
            if better_models:
                print(f"\nğŸ… æ‰¾åˆ° {len(better_models)} å€‹æ¯” Whisper-1 æ›´å¥½çš„æ¨¡å‹:")
                
                # æŒ‰ç¶œåˆè¡¨ç¾æ’åº
                better_models.sort(key=lambda x: x['analysis']['usability_score'], reverse=True)
                
                for i, model in enumerate(better_models, 1):
                    analysis = model['analysis']
                    score_improvement = analysis['usability_score'] - whisper1_score
                    speed_improvement = ((whisper1_time - model['processing_time']) / whisper1_time) * 100
                    
                    print(f"  {i}. {model['model']} ({model['config']})")
                    print(f"     å¯¦ç”¨æ€§æå‡: +{score_improvement:.1f} åˆ†")
                    print(f"     é€Ÿåº¦æå‡: +{speed_improvement:.1f}%")
                    print(f"     ç†æƒ³æ®µè½: {analysis['ideal_count']} å€‹ ({analysis['ideal_ratio']*100:.1f}%)")
                    print(f"     éé•·æ®µè½: {analysis['too_long_30_count']} å€‹")
                
                # æœ€çµ‚æ¨è–¦
                best_model = better_models[0]
                print(f"\nğŸ† æœ€ä½³æ¨è–¦: {best_model['model']} ({best_model['config']})")
                print(f"   æ¯” Whisper-1 å¥½ {best_model['analysis']['usability_score'] - whisper1_score:.1f} åˆ†")
                print(f"   {best_model['analysis']['rating']}")
                
            else:
                print(f"\nğŸ˜ æ²’æœ‰æ‰¾åˆ°æ¯” Whisper-1 æ›´å¥½çš„æ¨¡å‹")
                print(f"   æ‰€æœ‰æ¸¬è©¦æ¨¡å‹çš„è¡¨ç¾éƒ½ä¸å¦‚ Whisper-1")
    
    print(f"\nğŸ‰ é‡æ–°å¯¦æ¸¬å®Œæˆï¼")
    return results

if __name__ == "__main__":
    main()
